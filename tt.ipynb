{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/facestylor/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-04-07 11:59:00.220983: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-07 11:59:00.420082: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-04-07 11:59:01.150449: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/lib:/usr/local/cuda-11.8/lib64/:/usr/local/lib\n",
      "2023-04-07 11:59:01.150542: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/lib:/usr/local/cuda-11.8/lib64/:/usr/local/lib\n",
      "2023-04-07 11:59:01.150553: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel,BertConfig\n",
    "from config import Config\n",
    "from transformers import BertTokenizer\n",
    "config = Config('datas')\n",
    "tokenizer = BertTokenizer.from_pretrained('pretrained_bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class nnw(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, max_length):\n",
    "        super(nnw, self).__init__()\n",
    "        self.token_embeddings = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.position_embeddings = nn.Embedding(max_length, embedding_size)\n",
    "        self.layer_norm = nn.LayerNorm(embedding_size, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, input_ids):\n",
    "        print(input_ids.shape)\n",
    "        # 计算词向量\n",
    "        token_embeddings = self.token_embeddings(input_ids)\n",
    "        print(token_embeddings.shape)\n",
    "        \n",
    "        # 计算位置向量\n",
    "        batch_size, max_length = input_ids.shape\n",
    "        position_ids = torch.arange(max_length, dtype=torch.long, device=input_ids.device)\n",
    "        position_ids = position_ids.unsqueeze(0).repeat(batch_size, 1)\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        \n",
    "        # 将词向量和位置向量相加，并进行Layer Normalization和Dropout\n",
    "        embeddings = token_embeddings + position_embeddings\n",
    "        embeddings = self.layer_norm(embeddings)\n",
    "        print(embeddings.shape)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        print(embeddings.shape)\n",
    "        \n",
    "        return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128])\n",
      "torch.Size([1, 128, 768])\n",
      "torch.Size([1, 128, 768])\n",
      "torch.Size([1, 128, 768])\n",
      "Input text: This is an example sentence.\n",
      "Word embeddings shape: torch.Size([1, 128, 768])\n",
      "First word embedding: tensor([ 4.5158e-01, -1.6073e-01,  1.5860e-01,  1.6770e-01, -3.0257e-01,\n",
      "        -2.7660e-01, -5.0536e-01, -0.0000e+00,  3.2231e-01, -2.7956e-01,\n",
      "         2.1789e-01, -2.0754e-02, -1.7128e+00, -1.7044e+00, -1.0461e+00,\n",
      "        -9.0230e-01,  2.0175e-01,  6.8229e-01,  0.0000e+00,  0.0000e+00,\n",
      "         8.8319e-01, -0.0000e+00,  2.1701e+00, -2.8520e-01,  1.0690e-01,\n",
      "         2.0563e+00, -6.2606e-01,  9.5180e-02,  9.8283e-01,  3.4474e-01,\n",
      "         1.5007e-01, -2.8912e-01, -0.0000e+00, -9.6636e-01, -8.0026e-01,\n",
      "         0.0000e+00, -1.2651e+00,  2.1745e-01,  8.7802e-01,  4.4331e-01,\n",
      "        -9.5122e-01,  1.5882e+00, -1.3089e-01, -1.2789e+00, -1.3269e+00,\n",
      "         1.3040e+00, -0.0000e+00, -1.8494e-01, -0.0000e+00, -9.8907e-01,\n",
      "         2.1940e-02, -4.3158e-01,  7.8704e-01,  7.2779e-01, -1.7168e+00,\n",
      "         2.7894e+00, -1.2432e+00,  2.0469e-01,  1.1448e+00,  1.1234e+00,\n",
      "         0.0000e+00,  2.3132e+00,  5.1313e-01,  2.1867e+00,  0.0000e+00,\n",
      "        -1.1677e+00, -6.4312e-01,  8.6294e-01, -0.0000e+00, -3.2546e-01,\n",
      "        -9.2505e-01,  8.3090e-01, -1.0334e+00,  1.2037e+00, -1.2656e+00,\n",
      "         6.6840e-01,  5.0734e-01, -7.1738e-01,  1.4060e+00, -4.7630e-01,\n",
      "        -9.0533e-02, -1.0204e+00, -5.6241e-01,  1.2473e+00,  9.5580e-01,\n",
      "         7.7199e-01,  1.3268e+00,  0.0000e+00, -1.4447e+00,  1.6151e+00,\n",
      "         1.1039e+00,  4.6100e-01,  5.0734e-01, -2.8092e-01, -9.0834e-01,\n",
      "         1.1066e+00, -8.0426e-01, -1.3054e+00, -8.5687e-01,  1.7468e+00,\n",
      "         8.3297e-01, -1.0581e+00,  9.1282e-01,  4.8390e-01, -2.7163e+00,\n",
      "        -0.0000e+00, -1.3091e+00, -9.4642e-01,  7.0303e-01, -1.4752e-01,\n",
      "         5.4294e-01,  1.1988e-01,  2.8938e-01, -1.0210e+00, -6.6317e-01,\n",
      "         4.4294e-01, -4.0723e-01, -2.2956e-01, -0.0000e+00, -0.0000e+00,\n",
      "        -4.6993e-01, -0.0000e+00, -6.4914e-01, -1.2076e-01, -2.6102e-01,\n",
      "        -6.2910e-02,  3.0525e-01,  8.4488e-01, -9.0014e-02,  2.3303e-01,\n",
      "         1.6453e+00,  2.3263e-01, -2.4910e+00,  1.4688e+00, -6.6624e-02,\n",
      "         7.0953e-01, -1.7228e+00, -8.9001e-01,  1.5507e+00,  7.5411e-01,\n",
      "         2.3614e-02,  1.1735e-01,  1.5716e-01,  1.1459e+00,  0.0000e+00,\n",
      "         1.0933e+00, -1.0536e+00, -1.7467e+00, -7.4160e-01, -8.0783e-01,\n",
      "        -1.5765e+00,  2.1015e-01,  7.2200e-01,  1.0813e+00,  1.2322e-01,\n",
      "         2.3734e+00,  9.3073e-01,  7.2599e-01, -8.8949e-01, -2.1076e-01,\n",
      "        -1.1636e+00, -2.9033e-01,  7.0969e-01,  4.7714e-01, -1.6848e+00,\n",
      "         2.2873e-01,  1.2842e-01, -1.6525e+00, -0.0000e+00,  8.4001e-01,\n",
      "        -4.7932e-01,  3.2576e-01,  1.6034e+00,  7.1330e-01, -7.5968e-01,\n",
      "        -1.4997e-01, -2.4186e+00, -4.5884e-01, -5.2831e-01,  3.3036e-01,\n",
      "        -1.8331e+00,  2.0389e+00,  1.6526e-02, -6.8981e-01,  1.2805e+00,\n",
      "         1.9612e-01, -1.2291e+00,  0.0000e+00,  1.6398e+00,  1.2161e+00,\n",
      "        -0.0000e+00,  1.1387e+00,  2.6906e-01, -1.0879e+00, -0.0000e+00,\n",
      "         2.1637e+00,  1.7330e-02,  1.3101e+00,  6.5582e-01, -5.3536e-01,\n",
      "        -2.2015e-01, -1.2548e-01,  6.0215e-01,  1.9283e+00,  1.3320e-01,\n",
      "         1.5572e+00, -3.4944e-01, -4.9923e-01, -8.6138e-01, -9.9138e-02,\n",
      "        -2.2146e+00,  2.4708e-01,  1.4747e+00,  1.4824e+00,  1.1877e+00,\n",
      "        -1.9452e+00,  8.6327e-01,  5.4563e-01,  3.4268e-01, -0.0000e+00,\n",
      "         2.8287e+00,  2.4471e-01, -0.0000e+00,  2.3362e+00,  3.5113e-01,\n",
      "         3.6612e-01,  6.6926e-01, -6.8866e-01,  1.5924e+00, -2.2837e-02,\n",
      "        -0.0000e+00,  7.8278e-01, -9.1252e-01, -1.4250e+00, -9.3848e-01,\n",
      "        -9.6815e-01,  7.7597e-01,  2.0591e-01, -3.3148e-01, -2.1869e+00,\n",
      "        -1.2182e+00,  1.1135e+00, -7.9552e-02, -2.5738e-03, -1.4680e-01,\n",
      "        -2.1877e+00, -1.1840e+00, -1.5324e+00, -8.2405e-01,  0.0000e+00,\n",
      "         8.8322e-01,  4.9625e-01, -2.1735e+00, -2.0998e+00, -9.7330e-01,\n",
      "         1.6035e-01,  5.1115e-01,  9.3424e-01, -1.0307e-01, -8.6909e-01,\n",
      "        -1.1626e+00, -6.1018e-01,  0.0000e+00,  9.4093e-01, -8.3755e-01,\n",
      "         2.1519e+00, -6.3826e-01, -2.2514e-01,  1.2716e+00, -3.6393e-01,\n",
      "         2.1287e+00, -2.6416e-02, -1.3051e-01,  1.2268e-01,  6.5966e-02,\n",
      "         7.4025e-01,  1.3524e-01,  1.6165e-01, -4.5438e-01,  0.0000e+00,\n",
      "        -2.3534e+00, -7.9802e-02,  1.6747e+00, -4.9898e-02, -5.4732e-01,\n",
      "        -1.4034e+00, -3.7958e-01,  3.4449e+00, -5.9487e-01, -3.6130e-01,\n",
      "         2.3180e+00,  2.6900e-02,  0.0000e+00, -3.3597e-01,  1.3505e+00,\n",
      "        -4.9250e-01,  1.2431e+00,  1.3562e-01,  2.2595e+00, -0.0000e+00,\n",
      "         2.0392e-01, -3.6424e-01, -2.0003e-01,  2.6607e-01, -1.5478e+00,\n",
      "         2.8933e+00,  5.7489e-01, -3.7654e-01,  4.3258e-01, -1.5233e+00,\n",
      "        -8.5580e-01,  5.3371e-01, -9.9493e-01,  1.4657e+00, -5.3353e-01,\n",
      "         6.8790e-01,  1.8882e+00, -7.0515e-01,  0.0000e+00,  2.2051e+00,\n",
      "         1.6838e+00,  1.7658e-01,  2.9041e+00,  1.0099e+00, -0.0000e+00,\n",
      "         0.0000e+00, -0.0000e+00, -4.4598e-01, -8.3819e-02,  9.8798e-02,\n",
      "        -3.2861e-01,  2.2705e+00, -1.7614e+00, -2.2874e+00, -6.8620e-01,\n",
      "        -8.0918e-01,  6.6442e-01, -4.5829e-01, -1.9644e+00, -3.0939e-01,\n",
      "        -1.0665e+00, -1.0335e+00, -1.7888e+00, -0.0000e+00, -4.6246e-01,\n",
      "         1.0003e+00,  6.7197e-02, -3.1007e-01,  0.0000e+00,  1.2964e+00,\n",
      "         0.0000e+00,  7.1897e-01, -4.9777e-01,  9.8842e-01, -2.5487e-01,\n",
      "         1.2176e+00, -9.7511e-02, -9.7654e-01,  5.1894e-01, -8.2084e-01,\n",
      "        -1.2443e+00,  1.5477e+00, -1.8500e-01,  2.0102e-01, -0.0000e+00,\n",
      "         4.3155e-01, -6.5046e-01,  0.0000e+00, -6.2398e-01,  1.9556e-01,\n",
      "        -1.0085e+00, -9.5333e-01, -3.3548e-01, -2.1211e-01, -1.7534e-01,\n",
      "         1.1811e+00,  5.7232e-01, -1.2036e-01,  1.8003e+00, -1.4024e+00,\n",
      "        -2.8700e+00, -2.7783e-01, -9.5041e-01,  6.6897e-03,  0.0000e+00,\n",
      "        -0.0000e+00,  0.0000e+00, -2.0719e-01,  5.3791e-01,  1.1877e-01,\n",
      "        -0.0000e+00,  6.5062e-03, -3.3909e-01, -2.1441e+00, -8.8831e-01,\n",
      "         9.3381e-01,  1.9414e+00, -6.7283e-01,  1.8591e+00,  1.1946e-01,\n",
      "        -5.5199e-01, -1.9749e+00,  9.7648e-01,  1.9093e+00,  3.9262e-01,\n",
      "         4.7539e-01,  1.2952e+00, -2.1884e+00, -1.2372e+00, -8.7683e-01,\n",
      "         3.2370e-01,  5.3622e-01,  2.2767e+00,  3.8259e-01, -1.4272e-01,\n",
      "         2.9840e-01,  7.8349e-01, -1.5153e+00, -6.6586e-01, -1.2143e+00,\n",
      "        -1.0882e+00,  2.6544e+00, -1.3816e+00,  6.1148e-01, -8.4588e-02,\n",
      "        -1.4601e-01,  1.2991e-01,  1.8136e-01, -7.0926e-01,  4.4451e-01,\n",
      "        -1.7328e+00, -2.2805e+00,  2.5808e+00,  7.8214e-01,  1.3391e+00,\n",
      "        -1.8361e+00, -1.2371e+00,  9.4898e-01, -6.4630e-01,  0.0000e+00,\n",
      "         5.2195e-01,  3.2245e+00, -0.0000e+00, -1.8057e+00,  1.1607e+00,\n",
      "         5.7994e-01, -0.0000e+00, -0.0000e+00, -1.0255e+00, -4.9735e-01,\n",
      "         5.5790e-01, -6.6307e-01, -8.4158e-01, -1.6073e+00, -0.0000e+00,\n",
      "        -3.9089e-01, -5.6180e-01,  6.4100e-01,  5.2674e-01,  9.5509e-01,\n",
      "         2.5647e+00,  6.1812e-01,  2.4248e+00,  2.1378e+00, -8.4094e-01,\n",
      "        -1.1000e-01,  2.1184e+00, -5.2272e-01,  1.0422e+00,  7.8073e-01,\n",
      "        -4.3877e-01, -2.1269e+00, -8.1247e-01, -1.0774e+00,  1.2267e+00,\n",
      "        -6.9449e-01, -7.5374e-01, -0.0000e+00, -2.5029e-01, -1.2509e+00,\n",
      "        -1.0446e-01, -2.9030e+00, -2.2561e-01,  3.7243e-01, -8.3469e-01,\n",
      "         3.2489e-01, -0.0000e+00, -1.4269e+00,  4.5664e-01, -1.4727e+00,\n",
      "        -8.8852e-02,  1.0295e+00, -8.1365e-01, -2.4312e-01,  4.4552e-01,\n",
      "         1.6829e+00, -6.8160e-01, -1.7240e+00,  8.3030e-01, -1.1195e+00,\n",
      "         2.7992e-01,  2.1961e+00,  0.0000e+00, -2.9740e-02,  6.5364e-01,\n",
      "         3.9796e-01, -1.1082e+00, -7.3256e-01, -5.3859e-01, -1.2583e-01,\n",
      "         2.1138e-01, -2.0057e-01,  0.0000e+00, -5.4473e-01, -4.7429e-02,\n",
      "        -0.0000e+00,  7.2246e-01,  0.0000e+00,  2.4520e+00,  2.4531e+00,\n",
      "         7.3640e-02,  1.5222e-01,  2.6285e-02,  2.6950e-01,  4.5957e-01,\n",
      "         9.9798e-01, -1.5155e+00, -8.0053e-01, -1.7717e+00,  0.0000e+00,\n",
      "        -2.4953e+00,  8.6165e-01, -5.9903e-01,  2.8743e-01,  0.0000e+00,\n",
      "         0.0000e+00,  7.5614e-01,  0.0000e+00, -1.8040e+00,  2.2919e+00,\n",
      "        -6.1886e-01,  5.8813e-01,  0.0000e+00,  8.1133e-01, -1.4612e+00,\n",
      "        -1.8993e-01,  7.2834e-01, -8.8452e-01,  6.7275e-01, -7.8139e-01,\n",
      "        -8.6547e-01, -1.6983e+00,  6.6680e-01,  2.1081e+00,  2.0766e+00,\n",
      "        -1.4544e+00,  0.0000e+00, -2.9573e-01, -3.3701e-01, -0.0000e+00,\n",
      "        -0.0000e+00,  0.0000e+00,  2.5368e+00,  0.0000e+00, -5.8589e-01,\n",
      "         4.4244e-01, -6.2344e-01, -2.0754e-01, -6.4159e-02, -3.7439e-01,\n",
      "         6.0887e-01,  2.1680e+00, -9.7921e-01,  4.5315e-01,  6.1946e-02,\n",
      "         2.7372e-01,  5.5903e-01, -8.8957e-01, -8.4120e-01,  1.3531e+00,\n",
      "        -1.1096e+00, -3.5237e-01, -8.5611e-01, -0.0000e+00, -1.5693e+00,\n",
      "         4.8945e-01, -5.0787e-01, -1.0587e+00,  4.5449e-01,  6.3232e-02,\n",
      "        -3.8136e-01, -0.0000e+00,  5.3528e-01,  4.2716e-01,  6.6566e-01,\n",
      "         6.7130e-01,  3.8797e-01,  0.0000e+00, -0.0000e+00,  2.3623e-01,\n",
      "        -8.0685e-01, -1.2053e+00, -4.7378e-01, -2.7805e-01,  7.2155e-01,\n",
      "         1.5621e-01,  7.2297e-01, -9.4972e-02,  1.7572e-02,  2.6069e+00,\n",
      "        -0.0000e+00, -1.1397e+00, -0.0000e+00, -1.9664e+00, -8.9826e-02,\n",
      "        -1.0507e+00, -1.2326e+00,  1.1151e+00,  1.2248e+00, -5.9263e-01,\n",
      "        -8.1903e-01, -2.1350e+00, -2.2090e-01, -7.5450e-01, -6.3982e-02,\n",
      "         8.3021e-01,  1.6336e+00, -8.0595e-01,  1.4260e+00,  5.9380e-01,\n",
      "         1.5672e+00, -7.5382e-01, -8.0205e-01,  2.6875e-01,  1.5053e+00,\n",
      "        -1.6054e+00, -1.6738e+00,  4.0972e-01,  3.8637e-01,  4.9128e-01,\n",
      "         1.1303e+00, -1.8137e-01,  1.9416e+00,  2.6478e+00, -1.3902e+00,\n",
      "        -2.0439e-01,  3.2327e-01, -2.3458e-01,  5.2989e-01, -3.9485e-01,\n",
      "        -5.4378e-01,  1.4152e+00, -4.3453e-01,  4.9672e-01,  1.7665e-01,\n",
      "         1.0058e+00,  5.2449e-01, -9.2909e-01,  1.2347e+00, -1.7888e-01,\n",
      "         1.9138e-01,  6.3264e-01,  6.7537e-01,  0.0000e+00, -1.9413e+00,\n",
      "         1.2660e-01,  6.7932e-01, -1.0251e+00, -4.8108e-01, -8.6410e-01,\n",
      "        -1.0392e+00,  0.0000e+00, -5.9866e-01, -2.3774e-01,  4.6809e-02,\n",
      "        -0.0000e+00, -1.0355e+00, -1.7780e-01, -1.3563e+00, -4.0141e-02,\n",
      "         1.1277e+00, -2.3741e+00, -0.0000e+00,  5.3599e-01, -4.7449e-01,\n",
      "         1.3162e+00,  5.1822e-01, -1.9598e+00,  5.7040e-01, -6.5507e-01,\n",
      "        -2.6903e-01,  1.1206e+00,  1.0519e+00,  1.2224e-01, -2.7010e-01,\n",
      "        -2.8145e-01, -3.7432e-01, -1.5353e+00,  4.5752e-01, -1.8534e+00,\n",
      "        -1.7671e+00,  6.7623e-01, -8.1541e-01, -3.4932e-01, -3.8649e-01,\n",
      "         0.0000e+00, -2.2637e-01, -5.7075e-01,  0.0000e+00, -5.4123e-01,\n",
      "        -1.6563e+00, -5.8629e-01, -1.8096e+00,  0.0000e+00,  1.3968e-01,\n",
      "         2.1804e+00, -2.7254e-01, -2.0374e+00,  0.0000e+00, -0.0000e+00,\n",
      "         5.5118e-01, -7.2034e-01,  1.9009e+00,  1.9578e-01, -3.4133e+00,\n",
      "         1.4070e+00, -6.3088e-01,  2.0519e-01,  2.1521e-01,  1.6196e+00,\n",
      "        -1.3430e+00, -1.6703e+00,  1.6664e+00, -1.2748e+00, -6.1778e-01,\n",
      "         4.1727e-01, -2.1888e+00,  0.0000e+00,  2.7070e-01, -1.9872e+00,\n",
      "         0.0000e+00, -3.4488e-01,  8.0443e-01, -7.7342e-01, -2.5986e-01,\n",
      "        -4.4448e-01,  2.7242e-02, -1.1370e-01, -1.2496e+00,  5.2058e-05,\n",
      "        -2.6630e+00,  1.0797e+00, -1.2113e+00,  1.3814e+00,  3.0889e-01,\n",
      "         0.0000e+00, -1.2401e+00,  0.0000e+00, -5.7403e-01, -9.3908e-01,\n",
      "         4.6765e-01,  1.9711e-01,  9.0779e-01, -1.0466e+00,  1.2122e+00,\n",
      "         7.0210e-01, -2.9536e-01,  2.4696e+00])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# 加载预训练模型和tokenizer\n",
    "model_name = './pretrained_bert'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "# model = BertModel.from_pretrained(model_name)\n",
    "model = nnw(21128, 768, 256)\n",
    "\n",
    "# 准备输入数据\n",
    "text = \"This is an example sentence.\"\n",
    "encoded = tokenizer.encode_plus(text, add_special_tokens=True, \n",
    "                                padding='max_length', truncation=True, \n",
    "                                max_length=128, return_tensors='pt')\n",
    "input_ids = encoded['input_ids']\n",
    "attention_mask = encoded['attention_mask']\n",
    "\n",
    "# 获取词嵌入\n",
    "with torch.no_grad():\n",
    "    # outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    embeddings = model(input_ids)\n",
    "    # embeddings = outputs[0]\n",
    "\n",
    "print(f'Input text: {text}')\n",
    "print(f'Word embeddings shape: {embeddings.shape}')\n",
    "print(f'First word embedding: {embeddings[0][1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.batch_encode_plus(\n",
    "        ['text'],\n",
    "        padding=\"max_length\",\n",
    "        max_length=256,\n",
    "        truncation=\"longest_first\",\n",
    "        return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101, 10539,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dev_iterator = DataProcessor(config, tokenizer, 334, mode='dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3072"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "12*256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['kk','fd'],data=[[23,2],[45,34],[2,234]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kk</th>\n",
       "      <th>fd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>234</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   kk   fd\n",
       "0  23    2\n",
       "1  45   34\n",
       "2   2  234"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(df['kk'],pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = pd.DataFrame(df['kk'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd = {\n",
    "  '付费方式': [\"自费\", \"保险\", \"医疗救助\", \"医疗补助\", \"公费医疗\", \"自费和医保结合\"],\n",
    "  '状况状态': [\"正常\", \"异常\", \"稳定\", \"不稳定\", \"恶化\", \"好转\", \"未知\", \"待定\", \"无\", \"有\", \"确定\", \"疑似\", \"确诊\", \"未诊断\", \"建议隔离\", \"隔离期\", \"结束隔离\", \"重症\", \"轻症\", \n",
    "           \"中度症状\", \"失访\", \"死亡\"],\n",
    "  '婚姻状况': [\"已婚\", \"未婚\", \"离异\", \"丧偶\", \"复婚\"],\n",
    "  '就业状态': [\"在职\", \"离职\", \"退休\", \"自雇\", \"无业\", \"学生\", \"实习\"],\n",
    "  '宗教信仰': ['基督教', '佛教', '伊斯兰教', '印度教', '犹太教', '道教', '天主教', '东正教', '基督复临安息日会', '耶和华见证人', '摩门教', '神道教', '锡克教', '拜火教', '卡巴拉', \n",
    "           '苦行派', '巫教', '新教','一神论', '多神论', '无神论', '泛神论', '全在神中论', '不可知论', '唯一神论', '有神论', '灵性主义', '万物有灵论', '图腾崇拜', '祖先崇拜'],\n",
    "  '检查检验结果': ['血糖水平高', '血压偏高', '心电图异常', '白细胞计数升高', 'C-反应蛋白升高', '尿蛋白阳性', '肝功能异常', '血脂异常', '肺功能受损'],\n",
    "  '麻醉方式': ['全身麻醉', '局部麻醉', '表面麻醉', '脊麻醉', '硬膜外麻醉', '静脉麻醉'],\n",
    "  '麻醉效果': ['无痛', '轻度疼痛', '中度疼痛', '重度疼痛', '麻痹'],\n",
    "  '血型': ['A型', 'B型', 'AB型', 'O型','Rh阳性', 'Rh阴性'],\n",
    "  '药物状态': ['未开封', '已开封', '已过期'],\n",
    "  '用药途径':['口服', '外用', '注射', '鼻喉喷雾', '肛门给药', '阴道给药', '眼用'],\n",
    "  '影像名称': ['X线胸透', 'CT扫描', 'MRI检查', '超声心动图', '肺功能检查','乳腺X线检查', '胃肠镜检查', '骨密度检查', '眼底照相', '彩超检查', '心电图'],\n",
    "  '影像类型': ['CT扫描', 'MRI', '彩超', 'PET扫描', '血管造影'],\n",
    "\n",
    "  \n",
    "  '分娩方式': ['自然分娩', '产钳助产', '产瘤助产', '剖腹产'],\n",
    "  '乳房检查': ['乳腺触诊', '乳腺X线摄影', '乳房超声', '磁共振成像', '乳腺活检'],\n",
    "  '胎方位': ['头先露', '臀先露', '横位', '斜位', '足先露']\n",
    "\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['付费方式', '状况状态', '婚姻状况', '就业状态', '宗教信仰', '检查检验结果', '麻醉方式', '麻醉效果', '血型', '药物状态', '用药途径', '影像名称', '影像类型', '分娩方式', '乳房检查', '胎方位'])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('自费', '正常', '已婚', '在职', '基督教', '血糖水平高', '全身麻醉', '无痛', 'A型', '未开封', '口服', 'X线胸透', 'CT扫描', '自然分娩', '乳腺触诊', '头先露'), ('保险', '异常', '未婚', '离职', '佛教', '血压偏高', '局部麻醉', '轻度疼痛', 'B型', '已开封', '外用', 'CT扫描', 'MRI', '产钳助产', '乳腺X线摄影', '臀先露'), ('医疗救助', '稳定', '离异', '退休', '伊斯兰教', '心电图异常', '表面麻醉', '中度疼痛', 'AB型', '已过期', '注射', 'MRI检查', '彩超', '产瘤助产', '乳房超声', '横位'), ('医疗补助', '不稳定', '丧偶', '自雇', '印度教', '白细胞计数升高', '脊麻醉', '重度疼痛', 'O型', '', '鼻喉喷雾', '超声心动图', 'PET扫描', '剖腹产', '磁共振成像', '斜位'), ('公费医疗', '恶化', '复婚', '无业', '犹太教', 'C-反应蛋白升高', '硬膜外麻醉', '麻痹', 'Rh阳性', '', '肛门给药', '肺功能检查', '血管造影', '', '乳腺活检', '足先露'), ('自费和医保结合', '好转', '', '学生', '道教', '尿蛋白阳性', '静脉麻醉', '', 'Rh阴性', '', '阴道给药', '乳腺X线检查', '', '', '', ''), ('', '未知', '', '实习', '天主教', '肝功能异常', '', '', '', '', '眼用', '胃肠镜检查', '', '', '', ''), ('', '待定', '', '', '东正教', '血脂异常', '', '', '', '', '', '骨密度检查', '', '', '', ''), ('', '无', '', '', '基督复临安息日会', '肺功能受损', '', '', '', '', '', '眼底照相', '', '', '', ''), ('', '有', '', '', '耶和华见证人', '', '', '', '', '', '', '彩超检查', '', '', '', ''), ('', '确定', '', '', '摩门教', '', '', '', '', '', '', '心电图', '', '', '', ''), ('', '疑似', '', '', '神道教', '', '', '', '', '', '', '', '', '', '', ''), ('', '确诊', '', '', '锡克教', '', '', '', '', '', '', '', '', '', '', ''), ('', '未诊断', '', '', '拜火教', '', '', '', '', '', '', '', '', '', '', ''), ('', '建议隔离', '', '', '卡巴拉', '', '', '', '', '', '', '', '', '', '', ''), ('', '隔离期', '', '', '苦行派', '', '', '', '', '', '', '', '', '', '', ''), ('', '结束隔离', '', '', '巫教', '', '', '', '', '', '', '', '', '', '', ''), ('', '重症', '', '', '新教', '', '', '', '', '', '', '', '', '', '', ''), ('', '轻症', '', '', '一神论', '', '', '', '', '', '', '', '', '', '', ''), ('', '中度症状', '', '', '多神论', '', '', '', '', '', '', '', '', '', '', ''), ('', '失访', '', '', '无神论', '', '', '', '', '', '', '', '', '', '', ''), ('', '死亡', '', '', '泛神论', '', '', '', '', '', '', '', '', '', '', ''), ('', '', '', '', '全在神中论', '', '', '', '', '', '', '', '', '', '', ''), ('', '', '', '', '不可知论', '', '', '', '', '', '', '', '', '', '', ''), ('', '', '', '', '唯一神论', '', '', '', '', '', '', '', '', '', '', ''), ('', '', '', '', '有神论', '', '', '', '', '', '', '', '', '', '', ''), ('', '', '', '', '灵性主义', '', '', '', '', '', '', '', '', '', '', ''), ('', '', '', '', '万物有灵论', '', '', '', '', '', '', '', '', '', '', ''), ('', '', '', '', '图腾崇拜', '', '', '', '', '', '', '', '', '', '', ''), ('', '', '', '', '祖先崇拜', '', '', '', '', '', '', '', '', '', '', '')]\n"
     ]
    }
   ],
   "source": [
    "from itertools import zip_longest\n",
    "\n",
    "transposed = list(zip_longest(*fd.values(), fillvalue=''))\n",
    "\n",
    "print(transposed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(columns=fd.keys(),data=fd.values())\n",
    "df = pd.DataFrame(columns=fd.keys(),data=transposed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.159477233886719"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5410104/1024/1024"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.15 ('facestylor')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5ff31606606aa090868ed2268212262a530693e0bbdf1d34e55e824e0a387c5f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
